{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alaa7amid/AI/blob/main/clusteringEstimate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ne9vcGJYWaq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.lines import Line2D"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2M8ufkhhSAGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAC-lnIbYWas"
      },
      "outputs": [],
      "source": [
        "class DataCloud:\n",
        "\tN=0\n",
        "\tdef __init__(self,x):\n",
        "\t\tself.n=1\n",
        "\t\tself.mean=x\n",
        "\t\tself.variance=0\n",
        "\t\tself.pertinency=1\n",
        "\t\tDataCloud.N+=1\n",
        "\tdef addDataClaud(self,x):\n",
        "\t\tself.n=2\n",
        "\t\tself.mean=(self.mean+x)/2\n",
        "\t\tself.variance=((np.linalg.norm(self.mean-x))**2)\n",
        "\tdef updateDataCloud(self,n,mean,variance):\n",
        "\t\tself.n=n\n",
        "\t\tself.mean=mean\n",
        "\t\tself.variance=variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jZF3mG6YWat"
      },
      "outputs": [],
      "source": [
        "class AutoCloud:\n",
        "\tc= np.array([DataCloud(0)],dtype=DataCloud)\n",
        "\talfa= np.array([0.0],dtype=float)\n",
        "\tintersection = np.zeros((1,1),dtype=int)\n",
        "\tlistIntersection = np.zeros((1),dtype=int)\n",
        "\tmatrixIntersection = np.zeros((1,1),dtype=int)\n",
        "\trelevanceList = np.zeros((1),dtype=int)\n",
        "\tk=1\n",
        "\tdef __init__(self, m):\n",
        "\t\tAutoCloud.m = m\n",
        "\t\tAutoCloud.c = np.array([DataCloud(0)],dtype=DataCloud)\n",
        "\t\tAutoCloud.alfa= np.array([0.0],dtype=float)\n",
        "\t\tAutoCloud.intersection = np.zeros((1,1),dtype=int)\n",
        "\t\tAutoCloud.listIntersection = np.zeros((1),dtype=int)\n",
        "\t\tAutoCloud.relevanceList = np.zeros((1),dtype=int)\n",
        "\t\tAutoCloud.matrixIntersection = np.zeros((1,1),dtype=int)\n",
        "\t\tAutoCloud.k=1\n",
        "\t\tAutoCloud.classIndex = [[1.0],[1.0]]\n",
        "\t\tAutoCloud.clusterIndex = [0, 0]\n",
        "\n",
        "\tdef mergeClouds(self):\n",
        "\t\ti=0\n",
        "\t\twhile(i<len(AutoCloud.listIntersection)-1):\n",
        "\t\t\tmerge=False\n",
        "\t\t\tj=i+1\n",
        "\t\t\twhile(j<len(AutoCloud.listIntersection)):\n",
        "\t\t\t\t#print(\"i\",i,\"j\",j,\"l\",np.size(AutoCloud.listIntersection),\"m\",np.size(AutoCloud.matrixIntersection),\"c\",np.size(AutoCloud.c))\n",
        "\t\t\t\tif(AutoCloud.listIntersection[i] == 1 and AutoCloud.listIntersection[j] == 1):\n",
        "\t\t\t\t\tAutoCloud.matrixIntersection[i,j] = AutoCloud.matrixIntersection[i,j] + 1;\n",
        "\t\t\t\tnI = AutoCloud.c[i].n\n",
        "\t\t\t\tnJ = AutoCloud.c[j].n\n",
        "\t\t\t\tmeanI = AutoCloud.c[i].mean\n",
        "\t\t\t\tmeanJ = AutoCloud.c[j].mean\n",
        "\t\t\t\tvarianceI = AutoCloud.c[i].variance\n",
        "\t\t\t\tvarianceJ = AutoCloud.c[j].variance\n",
        "\t\t\t\tnIntersc = AutoCloud.matrixIntersection[i,j]\n",
        "\t\t\t\tif (nIntersc > (nI - nIntersc) or nIntersc > (nJ - nIntersc)):\n",
        "\t\t\t\t\tmerge = True\n",
        "\t\t\t\t\t#update values\n",
        "\t\t\t\t\tn = nI + nJ - nIntersc\n",
        "\t\t\t\t\tmean = ((nI * meanI) + (nJ * meanJ))/(nI + nJ)\n",
        "\t\t\t\t\tvariance = ((nI - 1) * varianceI + (nJ - 1) * varianceJ)/(nI + nJ - 2)\n",
        "\t\t\t\t\tnewCloud = DataCloud(mean)\n",
        "\t\t\t\t\tnewCloud.updateDataCloud(n,mean,variance)\n",
        "\t\t\t\t\t#atualizando lista de interseção\n",
        "\t\t\t\t\tAutoCloud.listIntersection = np.concatenate((AutoCloud.listIntersection[0 : i], np.array([1]), AutoCloud.listIntersection[i + 1 : j],AutoCloud.listIntersection[j + 1 : np.size(AutoCloud.listIntersection)]),axis=None)\n",
        "\t\t\t\t\t#atualizando lista de data clouds\n",
        "\t\t\t\t\tAutoCloud.c = np.concatenate((AutoCloud.c[0 : i ], np.array([newCloud]), AutoCloud.c[i + 1 : j],AutoCloud.c[j + 1 : np.size(AutoCloud.c)]),axis=None)\n",
        "\t\t\t\t\t#update  intersection matrix\n",
        "\t\t\t\t\tM0 = AutoCloud.matrixIntersection\n",
        "\t\t\t\t\t#Remover linhas\n",
        "\t\t\t\t\tM1=np.concatenate((M0[0 : i , :],np.zeros((1,len(M0))),M0[i + 1 : j, :],M0[j + 1 : len(M0), :]))\n",
        "\t\t\t\t\t#remover colunas\n",
        "\t\t\t\t\tM1=np.concatenate((M1[:, 0 : i ],np.zeros((len(M1),1)),M1[:, i+1 : j],M1[:, j+1 : len(M0)]),axis=1)\n",
        "\t\t\t\t\t#calculando nova coluna\n",
        "\t\t\t\t\tcol = (M0[:, i] + M0[:, j])*(M0[: , i]*M0[:, j] != 0)\n",
        "\t\t\t\t\tcol = np.concatenate((col[0 : j], col[j + 1 : np.size(col)]))\n",
        "\t\t\t\t\t#calculando nova linha\n",
        "\t\t\t\t\tlin = (M0[i, :]+M0[j, :])*(M0[i, :]*M0[j, :] != 0)\n",
        "\t\t\t\t\tlin = np.concatenate((lin[ 0 : j], lin[j + 1 : np.size(lin)]))\n",
        "\t\t\t\t\t#atualizando coluna\n",
        "\t\t\t\t\tM1[:,i]=col\n",
        "\t\t\t\t\t#atualizando linha\n",
        "\t\t\t\t\tM1[i,:]=lin\n",
        "\t\t\t\t\tM1[i, i + 1 : j] = M0[i, i + 1 : j] + M0[i + 1 : j, j].T;\n",
        "\t\t\t\t\tAutoCloud.matrixIntersection = M1\n",
        "\t\t\t\tj += 1\n",
        "\t\t\tif(merge):\n",
        "\t\t\t\ti = 0\n",
        "\t\t\telse:\n",
        "\t\t\t\ti += 1\n",
        "\n",
        "\tdef run(self,X):\n",
        "\t\tAutoCloud.listIntersection = np.zeros((np.size(AutoCloud.c)),dtype=int)\n",
        "\t\tif AutoCloud.k==1:\n",
        "\t\t\tAutoCloud.c[0]=DataCloud(X)\n",
        "\n",
        "\t\telif AutoCloud.k==2:\n",
        "\t\t\tAutoCloud.c[0].addDataClaud(X)\n",
        "\t\telif AutoCloud.k>=3:\n",
        "\t\t\ti=0\n",
        "\t\t\tcreateCloud = True\n",
        "\t\t\tAutoCloud.alfa = np.zeros((np.size(AutoCloud.c)),dtype=float)\n",
        "\t\t\tfor data in AutoCloud.c:\n",
        "\t\t\t\tn= data.n +1\n",
        "\t\t\t\tmean = ((n-1)/n)*data.mean + (1/n)*X\n",
        "\t\t\t\tvariance = ((n-1)/n)*data.variance +(1/n)*((np.linalg.norm(X-mean))**2)\n",
        "\t\t\t\teccentricity=(1/n)+((mean-X).T.dot(mean-X))/(n*variance)\n",
        "\t\t\t\ttypicality = 1 - eccentricity\n",
        "\t\t\t\tnorm_eccentricity = eccentricity/2\n",
        "\t\t\t\tnorm_typicality = typicality/(AutoCloud.k-2)\n",
        "\t\t\t\tif(norm_eccentricity<=(AutoCloud.m**2 +1)/(2*n)):\n",
        "\t\t\t\t\tdata.updateDataCloud(n,mean,variance)\n",
        "\t\t\t\t\tAutoCloud.alfa[i] = norm_typicality\n",
        "\t\t\t\t\tcreateCloud= False\n",
        "\t\t\t\t\tAutoCloud.listIntersection.itemset(i,1)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tAutoCloud.alfa[i] = norm_typicality\n",
        "\t\t\t\t\tAutoCloud.listIntersection.itemset(i,0)\n",
        "\t\t\t\ti+=1\n",
        "\n",
        "\t\t\tif(createCloud):\n",
        "\t\t\t\tAutoCloud.c = np.append(AutoCloud.c,DataCloud(X))\n",
        "\t\t\t\tAutoCloud.listIntersection = np.insert(AutoCloud.listIntersection,i,1)\n",
        "\t\t\t\tAutoCloud.matrixIntersection = np.pad(AutoCloud.matrixIntersection, ((0,1),(0,1)), 'constant', constant_values=(0))\n",
        "\t\t\tself.mergeClouds()\n",
        "\t\t\tAutoCloud.relevanceList = AutoCloud.alfa /np.sum(AutoCloud.alfa)\n",
        "\t\t\tAutoCloud.clusterIndex.append(np.argmax(AutoCloud.relevanceList))\n",
        "\t\t\tAutoCloud.classIndex.append(AutoCloud.alfa)\n",
        "\n",
        "\n",
        "\t\tAutoCloud.k=AutoCloud.k+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qTRWdULYWau"
      },
      "outputs": [],
      "source": [
        "def Dmin(c):\n",
        "\tdmin = np.linalg.norm(np.subtract(c[0].mean,c[1].mean))\n",
        "\tfor i in range(len(c)):\n",
        "\t\tfor j in range(len(c)):\n",
        "\t\t\tif(i != j):\n",
        "\t\t\t\tif(np.linalg.norm(np.subtract(c[i].mean,c[j].mean))<dmin):\n",
        "\t\t\t\t\tdmin = np.linalg.norm(np.subtract(c[i].mean,c[j].mean))\n",
        "\treturn dmin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQcexkbiYWav"
      },
      "source": [
        "## 1. Spiral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5k02474YWaw"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/AutoCloud-master (5)/Spiral.csv')\n",
        "data = data.dropna()\n",
        "#dados = np.array([data[\"1\"],data[\"2\"]])\n",
        "#dados= dados.T\n",
        "#class_labels = np.array(data[\"3\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri5b1yt3YWaw"
      },
      "outputs": [],
      "source": [
        "\n",
        "teste=AutoCloud(2)\n",
        "purity=[]\n",
        "accuracy=[]\n",
        "NMI=[]\n",
        "for t in dados:\n",
        "    teste.run(np.array(t))\n",
        "    if teste.k>2:\n",
        "        max_idx=max(teste.clusterIndex)\n",
        "        indexed_data=pd.DataFrame(class_labels[0:teste.k-1],columns=[\"classLabel\"])\n",
        "        indexed_data[\"clusterLabel\"]=teste.clusterIndex\n",
        "        sorted_data=indexed_data.value_counts().to_frame()\n",
        "        C=sorted_data.groupby([\"clusterLabel\"]).sum() # clustered number of data\n",
        "        n_d=[]\n",
        "        precision=[]\n",
        "        HY_c=[]\n",
        "        for idx in range(max_idx+1):\n",
        "            if (1,idx) not in sorted_data.index:\n",
        "                tmp1=0\n",
        "                tmp2=sorted_data.loc[(2,idx),0]\n",
        "                p_c2=tmp2/(tmp1+tmp2)\n",
        "                HY_c_tmp=-C.loc[idx,0]/C.sum()[0]*p_c2*np.log2(p_c2)\n",
        "            elif (2,idx) not in sorted_data.index:\n",
        "                tmp1=sorted_data.loc[(1,idx),0]\n",
        "                tmp2=0\n",
        "                p_c1=tmp1/(tmp1+tmp2)\n",
        "                HY_c_tmp=-C.loc[idx,0]/C.sum()[0]*p_c1*np.log2(p_c1)\n",
        "            else:\n",
        "                tmp1=sorted_data.loc[(1,idx),0]\n",
        "                tmp2=sorted_data.loc[(2,idx),0]\n",
        "                p_c1=tmp1/(tmp1+tmp2)\n",
        "                p_c2=tmp2/(tmp1+tmp2)\n",
        "                HY_c_tmp=-C.loc[idx,0]/C.sum()[0]*(p_c1*np.log2(p_c1)+p_c2*np.log2(p_c2))\n",
        "            n_d.append(max(tmp1,tmp2))\n",
        "            precision.append(max(tmp1,tmp2)/C.loc[idx,0])\n",
        "            HY_c.append(HY_c_tmp)\n",
        "        purity.append(sum(n_d)/teste.k)\n",
        "        accuracy.append(np.mean(precision))\n",
        "\n",
        "        # entropy of class labels\n",
        "        Y=sorted_data.groupby([\"classLabel\"]).sum()\n",
        "        p=Y/np.sum(Y)\n",
        "        HY=-np.sum(p*np.log2(p))[0]\n",
        "        # entropy of cluster labels\n",
        "        p=C/np.sum(C)\n",
        "        HC=-np.sum(p*np.log2(p))[0]\n",
        "        # mutual information\n",
        "        I=HY-sum(HY_c)\n",
        "        # Thus, the NMI\n",
        "        NMI.append(2*I/(HY+HC))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ixr1NLtuYWax"
      },
      "outputs": [],
      "source": [
        "sampleSpeed=10 # samples per time unit\n",
        "t=np.linspace(0,len(purity)/sampleSpeed,len(purity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD-nwpW7YWax"
      },
      "outputs": [],
      "source": [
        "sample_speed=[5,10,15,20,25]\n",
        "avgPurity=[]\n",
        "for speed in sample_speed:\n",
        "    tmp=[]\n",
        "    for i in range(1,len(purity)//speed):\n",
        "        tmp.append(purity[speed*i])\n",
        "    avgPurity.append(sum(tmp)/len(tmp)*100) # [%]\n",
        "\n",
        "# creating the bar plot\n",
        "plt.bar(sample_speed, avgPurity, color = 'blue', width = 3)\n",
        "plt.xlabel(\"Sample Speed (PPS)\")\n",
        "plt.ylabel(\"Avg Purity (%)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZy9YQSKYWax"
      },
      "outputs": [],
      "source": [
        "# purity\n",
        "plt.plot(t,purity)\n",
        "plt.ylim([0,1])\n",
        "plt.xlabel('Time [unit]')\n",
        "plt.ylabel(\"Purity\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qld5ge52YWay"
      },
      "outputs": [],
      "source": [
        "print(\"Purity:\")\n",
        "print(purity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9ZfQ9EUYWay"
      },
      "outputs": [],
      "source": [
        "# accuracy\n",
        "plt.plot(t,accuracy)\n",
        "plt.ylim([0,1])\n",
        "plt.xlabel('Time [unit]')\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show(purity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz9PzarJYWay"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy:\")\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uKn1LFRYWay"
      },
      "outputs": [],
      "source": [
        "# NMI\n",
        "plt.plot(t,NMI)\n",
        "plt.xlabel('Time [unit]')\n",
        "plt.ylabel(\"NMI\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Crxp8xaYWaz"
      },
      "outputs": [],
      "source": [
        "print(\"NMI:\")\n",
        "print(NMI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8IK1R9iYWaz"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (18,12)\n",
        "plt.grid()\n",
        "dados= dados.T\n",
        "plt.plot(dados[0],dados[1],'.g')\n",
        "for i in range(0,np.size(teste.c)):\n",
        "    plt.plot(teste.c[i].mean[0],teste.c[i].mean[1],'x',color='black')\n",
        "plt.legend(['Samples','Auto-Cloud'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlEVRW79YWaz"
      },
      "source": [
        "## 2. KDDCUP99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWoZlPAkYWaz"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/AutoCloud-master (5)/KDDCUP99.csv', header=None)\n",
        "data = data.dropna()\n",
        "dados = np.array(data.loc[:,0:41])\n",
        "class_labels = np.array(data.loc[:,42])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bVbm7tbYWaz"
      },
      "outputs": [],
      "source": [
        "teste=AutoCloud(42)\n",
        "purity=[]\n",
        "accuracy=[]\n",
        "NMI=[]\n",
        "for t in dados:\n",
        "    teste.run(np.array(t))\n",
        "    if teste.k>2:\n",
        "        max_idx=max(teste.clusterIndex)\n",
        "        indexed_data=pd.DataFrame(class_labels[0:teste.k-1],columns=[\"classLabel\"])\n",
        "        indexed_data[\"clusterLabel\"]=teste.clusterIndex\n",
        "        sorted_data=indexed_data.value_counts().to_frame()\n",
        "        C=sorted_data.groupby([\"clusterLabel\"]).sum() # clustered number of data\n",
        "        n_d=[]\n",
        "        precision=[]\n",
        "        HY_c=[]\n",
        "        for idx in range(max_idx+1):\n",
        "            if (1,idx) not in sorted_data.index:\n",
        "                tmp1=0\n",
        "                tmp2=sorted_data.loc[(2,idx),0]\n",
        "                p_c2=tmp2/(tmp1+tmp2)\n",
        "                HY_c_tmp=-C.loc[idx,0]/C.sum()[0]*p_c2*np.log2(p_c2)\n",
        "            elif (2,idx) not in sorted_data.index:\n",
        "                tmp1=sorted_data.loc[(1,idx),0]\n",
        "                tmp2=0\n",
        "                p_c1=tmp1/(tmp1+tmp2)\n",
        "                HY_c_tmp=-C.loc[idx,0]/C.sum()[0]*p_c1*np.log2(p_c1)\n",
        "            else:\n",
        "                tmp1=sorted_data.loc[(1,idx),0]\n",
        "                tmp2=sorted_data.loc[(2,idx),0]\n",
        "                p_c1=tmp1/(tmp1+tmp2)\n",
        "                p_c2=tmp2/(tmp1+tmp2)\n",
        "                HY_c_tmp=-C.loc[idx,0]/C.sum()[0]*(p_c1*np.log2(p_c1)+p_c2*np.log2(p_c2))\n",
        "            n_d.append(max(tmp1,tmp2))\n",
        "            precision.append(max(tmp1,tmp2)/C.loc[idx,0])\n",
        "            HY_c.append(HY_c_tmp)\n",
        "        purity.append(sum(n_d)/teste.k)\n",
        "        accuracy.append(np.mean(precision))\n",
        "\n",
        "        # entropy of class labels\n",
        "        Y=sorted_data.groupby([\"classLabel\"]).sum()\n",
        "        p=Y/np.sum(Y)\n",
        "        HY=-np.sum(p*np.log2(p))[0]\n",
        "        # entropy of cluster labels\n",
        "        p=C/np.sum(C)\n",
        "        HC=-np.sum(p*np.log2(p))[0]\n",
        "        # mutual information\n",
        "        I=HY-sum(HY_c)\n",
        "        # Thus, the NMI\n",
        "        NMI.append(2*I/(HY+HC))\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6ypPSB7YWaz"
      },
      "outputs": [],
      "source": [
        "sampleSpeed=10 # samples per time unit\n",
        "t=np.linspace(0,len(purity)/sampleSpeed,len(purity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyXt6khAYWaz"
      },
      "outputs": [],
      "source": [
        "sample_speed=[5,10,15,20,25]\n",
        "avgPurity=[]\n",
        "for speed in sample_speed:\n",
        "    tmp=[]\n",
        "    for i in range(1,len(purity)//speed):\n",
        "        tmp.append(purity[speed*i])\n",
        "    avgPurity.append(sum(tmp)/len(tmp)*100) # [%]\n",
        "\n",
        "# creating the bar plot\n",
        "plt.bar(sample_speed, avgPurity, color = 'blue', width = 3)\n",
        "plt.xlabel(\"Sample Speed (PPS)\")\n",
        "plt.ylabel(\"Avg Purity (%)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxylMaMbYWaz"
      },
      "outputs": [],
      "source": [
        "# purity\n",
        "plt.plot(t,purity)\n",
        "plt.ylim([0,1])\n",
        "plt.xlabel('Time [unit]')\n",
        "plt.ylabel(\"Purity\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbx85bboYWa0"
      },
      "outputs": [],
      "source": [
        "print(\"Purity:\")\n",
        "print(purity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZHAlmBZYWa0"
      },
      "outputs": [],
      "source": [
        "# accuracy\n",
        "plt.plot(t,accuracy)\n",
        "plt.ylim([0,1])\n",
        "plt.xlabel('Time [unit]')\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gTQMLjTYWa0"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy:\")\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdt78e6oYWa0"
      },
      "outputs": [],
      "source": [
        "# NMI\n",
        "plt.plot(t,NMI)\n",
        "plt.ylim([0,1])\n",
        "plt.xlabel('Time [unit]')\n",
        "plt.ylabel(\"NMI\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj94W3RUYWa0"
      },
      "outputs": [],
      "source": [
        "print(\"NMI:\")\n",
        "print(NMI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAaMEdebYWa0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}